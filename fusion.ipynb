{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb943d7-86d7-43d1-980d-1e526c623820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c126b-5255-4ce0-a5e9-8cf49fbe8076",
   "metadata": {},
   "source": [
    "Defining the data directories and csv file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79232746-a908-4055-bcc2-a143a98e4932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data/MURA-v1.1'\n",
    "train_image_paths_csv = \"train_image_paths.csv\"\n",
    "valid_image_paths_csv = \"valid_image_paths.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176a6e6-5ab9-491f-a1c2-8c9b7b7f566e",
   "metadata": {},
   "source": [
    "Reading the CSV file for training images and extracting the image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cdcab0-af18-4467-b00e-cb2c3f606cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images_paths = pd.read_csv(os.path.join(path,train_image_paths_csv),dtype=str,header=None)\n",
    "train_images_paths.columns = ['image_path']\n",
    "\n",
    "valid_images_paths = pd.read_csv(os.path.join(path,valid_image_paths_csv),dtype=str,header=None)\n",
    "valid_images_paths.columns = ['image_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ba9dd-9844-4767-a937-fba2e0d76f02",
   "metadata": {},
   "source": [
    "Extracting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635334ff-99a6-4346-b8b4-5ff04b43a58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images_paths['label'] = train_images_paths['image_path'].map(\n",
    "    lambda x: 1 if 'positive' in x else 0)\n",
    "\n",
    "valid_images_paths['label'] = valid_images_paths['image_path'].map(\n",
    "    lambda x: 1 if 'positive' in x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f7c51-bd7d-4025-adad-f689808d8c7d",
   "metadata": {},
   "source": [
    "Extracting other relevant information and displaying the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efc806f-fa6c-4fab-87ef-bc371256a8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>patientId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n",
       "      <td>1</td>\n",
       "      <td>XR_SHOULDER</td>\n",
       "      <td>00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n",
       "      <td>1</td>\n",
       "      <td>XR_SHOULDER</td>\n",
       "      <td>00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n",
       "      <td>1</td>\n",
       "      <td>XR_SHOULDER</td>\n",
       "      <td>00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n",
       "      <td>1</td>\n",
       "      <td>XR_SHOULDER</td>\n",
       "      <td>00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n",
       "      <td>1</td>\n",
       "      <td>XR_SHOULDER</td>\n",
       "      <td>00002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  label     category  \\\n",
       "0  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER   \n",
       "1  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER   \n",
       "2  MURA-v1.1/train/XR_SHOULDER/patient00001/study...      1  XR_SHOULDER   \n",
       "3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1  XR_SHOULDER   \n",
       "4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...      1  XR_SHOULDER   \n",
       "\n",
       "  patientId  \n",
       "0     00001  \n",
       "1     00001  \n",
       "2     00001  \n",
       "3     00002  \n",
       "4     00002  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_paths['category']  = train_images_paths['image_path'].apply(\n",
    "    lambda x: x.split('/')[2])  \n",
    "train_images_paths['patientId']  = train_images_paths['image_path'].apply(\n",
    "    lambda x: x.split('/')[3].replace('patient',''))\n",
    "\n",
    "valid_images_paths['category']  = valid_images_paths['image_path'].apply(\n",
    "    lambda x: x.split('/')[2])  \n",
    "valid_images_paths['patientId']  = valid_images_paths['image_path'].apply(\n",
    "    lambda x: x.split('/')[3].replace('patient',''))\n",
    "\n",
    "train_images_paths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98efe78-7aa4-442f-97b2-2682349602ac",
   "metadata": {},
   "source": [
    "Printing train dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d32b14d-5e16-40ad-8718-237afa065c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of images: 36808\n",
      "\n",
      "number of null values\n",
      " image_path    0\n",
      "label         0\n",
      "category      0\n",
      "patientId     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "categories:\n",
      "              count\n",
      "category          \n",
      "XR_WRIST      9752\n",
      "XR_SHOULDER   8379\n",
      "XR_HAND       5543\n",
      "XR_FINGER     5106\n",
      "XR_ELBOW      4931\n",
      "XR_FOREARM    1825\n",
      "XR_HUMERUS    1272\n",
      "\n",
      "\n",
      "number of patients: 11184\n",
      "\n",
      "number of labels: 2\n",
      "\n",
      "positive casses: 14873\n",
      "\n",
      "negative casses: 21935\n"
     ]
    }
   ],
   "source": [
    "total_number_of_training_images = np.shape(train_images_paths)[0]\n",
    "print(\"total number of images:\",total_number_of_training_images )\n",
    "print (\"\\nnumber of null values\\n\", train_images_paths.isnull().sum())\n",
    "\n",
    "\n",
    "categories_counts = pd.DataFrame(train_images_paths['category'].value_counts())\n",
    "print ('\\n\\ncategories:\\n',categories_counts )\n",
    "print('\\n\\nnumber of patients:',train_images_paths['patientId'].nunique())\n",
    "print('\\nnumber of labels:',train_images_paths['label'].nunique())\n",
    "print ('\\npositive casses:',len(train_images_paths[train_images_paths['label']==1]))\n",
    "print ('\\nnegative casses:',len(train_images_paths[train_images_paths['label']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54752f19-e90b-476a-b35c-d96c30a62f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category  XR_ELBOW  XR_FINGER  XR_FOREARM  XR_HAND  XR_HUMERUS  XR_SHOULDER  \\\n",
      "n_images                                                                      \n",
      "1               31        336          84       28          22          289   \n",
      "2              691        155         651      484         478          487   \n",
      "3              608       1172          91     1306          61          920   \n",
      "4              272        132          32       65          20          834   \n",
      "5               68         27           5       19           5           82   \n",
      "6               27         30           1       36           1           33   \n",
      "7               11          6           1        3           0           21   \n",
      "8                1          5           0        2           0           17   \n",
      "9                1          1           0        0           0            1   \n",
      "10               1          1           0        1           0            2   \n",
      "11               0          0           0        1           0            4   \n",
      "12               0          0           0        0           0            0   \n",
      "13               0          0           0        0           0            1   \n",
      "14               0          0           0        0           0            2   \n",
      "15               0          0           0        0           0            1   \n",
      "\n",
      "category  XR_WRIST  \n",
      "n_images            \n",
      "1              251  \n",
      "2              602  \n",
      "3             1778  \n",
      "4              449  \n",
      "5               56  \n",
      "6               88  \n",
      "7               19  \n",
      "8                8  \n",
      "9                8  \n",
      "10               3  \n",
      "11               0  \n",
      "12               5  \n",
      "13               0  \n",
      "14               0  \n",
      "15               0  \n"
     ]
    }
   ],
   "source": [
    "counts = (\n",
    "    train_images_paths\n",
    "    .groupby(['category', 'patientId'])\n",
    "    .size()\n",
    "    .reset_index(name='n_images')\n",
    ")\n",
    "\n",
    "dist = (\n",
    "    counts\n",
    "    .groupby(['category', 'n_images'])\n",
    "    .size()\n",
    "    .reset_index(name='n_patients')\n",
    "    .sort_values(['category', 'n_images'])\n",
    ")\n",
    "\n",
    "pivot = dist.pivot(\n",
    "    index='n_images', columns='category', values='n_patients').fillna(0).astype(int)\n",
    "\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb7837-baa8-4295-bb1c-ba00bb2babd6",
   "metadata": {},
   "source": [
    "Define augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18415d2d-581a-4217-bd9a-d08a8fe3b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tf = T.Compose([\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.Resize((256,256)),\n",
    "    T.RandomRotation(5),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])   # samplewise ≈ per-image norm\n",
    "])\n",
    "\n",
    "valid_tf = T.Compose([\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.Resize((256,256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59d87d-5b83-400b-90bc-641d85f7ea10",
   "metadata": {},
   "source": [
    "Define pytorch dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49bea1c-ced7-41b4-8290-a887336e5fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is one patient's full set of X-ray images plus their label.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Group by patient; keep the first label (assume all images share it)\n",
    "        self.patients = (\n",
    "            df.groupby(\"patientId\")\n",
    "            .agg(image_paths=(\"image_path\", list), label=(\"label\", \"first\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patients.iloc[idx]\n",
    "        imgs = []\n",
    "        for p in row[\"image_paths\"]:\n",
    "            img = Image.open(os.path.join(self.root_dir, p)).convert(\"L\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "\n",
    "        # (N, 1, H, W) — variable N per patient\n",
    "        images = torch.stack(imgs, dim=0)\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        return images, label\n",
    "\n",
    "\n",
    "def patient_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Returns a list of image tensors (one per patient) and a stacked label tensor.\n",
    "    \"\"\"\n",
    "    image_list = [item[0] for item in batch]   # list of (N_i, 1, H, W)\n",
    "    labels = torch.stack([item[1] for item in batch])  # (B,)\n",
    "    return image_list, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cebe58-cd17-4453-87a7-585e6a35eb32",
   "metadata": {},
   "source": [
    "Create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990cd17b-03d5-45e9-a65e-5ed5724a33e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_path = 'data'\n",
    "\n",
    "train_ds = PatientDataset(train_images_paths, parent_path, train_tf)\n",
    "valid_ds = PatientDataset(valid_images_paths, parent_path, valid_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True,\n",
    "                          collate_fn=patient_collate_fn)\n",
    "val_loader   = DataLoader(valid_ds, batch_size=16, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True,\n",
    "                          collate_fn=patient_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5f3f3-7d96-4ed4-a4dc-5ed5decbbc7d",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd5ecf9-175e-45ca-ac11-4bd05ce18f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths of x, y:\n",
      " 16 16\n",
      "shapes of x, y:\n",
      " torch.Size([2, 1, 256, 256]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "print('lengths of x, y:\\n', len(x), len(y))\n",
    "print('shapes of x, y:\\n', x[0].shape, y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741a6b16-2fc8-4496-afda-d7ba2c5120b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths of x, y:\n",
      " 16 16\n",
      "shapes of x, y:\n",
      " torch.Size([4, 1, 256, 256]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(val_loader))\n",
    "print('lengths of x, y:\\n', len(x), len(y))\n",
    "print('shapes of x, y:\\n', x[0].shape, y[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e77934-e28e-407b-bb44-3431b0176686",
   "metadata": {},
   "source": [
    "Custom Visual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be6e2f6-8fe6-45a8-8738-636e3d10bf53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from architectures.custom_vit import Custom_ViT\n",
    "#vit = Custom_ViT(img_size=256, patch_size=16, in_chans=1,\n",
    "#     embed_dim=256, depth=6, heads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e370ec-31ba-4884-884e-0650c013be61",
   "metadata": {},
   "source": [
    "Pretrained Resnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b7bf49a-60c5-422f-b5f9-3a34f80797f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from architectures.resnet50 import ResNet50_Backbone\n",
    "#backbone = ResNet50_Backbone(embed_dim=256, freeze_until='layer4')\n",
    "\n",
    "#from architectures.resnet101 import ResNet101_Backbone\n",
    "#backbone = ResNet101_Backbone(embed_dim=256, freeze_until='layer4')\n",
    "\n",
    "from architectures.resnet152 import ResNet152_Backbone\n",
    "backbone = ResNet152_Backbone(embed_dim=256, freeze_until='layer4')\n",
    "\n",
    "unfreeze_groups = [\n",
    "    backbone.backbone[7],\n",
    "    backbone.backbone[6],\n",
    "    backbone.backbone[5],\n",
    "    backbone.backbone[4],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f2d48-980b-48f2-aa07-54eaa2ab8717",
   "metadata": {},
   "source": [
    "Pretrained ViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995b7bf-e784-4232-8384-ca8f20c1c021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vit_b_16\n",
    "# vit_b_32\n",
    "# vit_l_16\n",
    "# vit_l_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a30ac-a3ff-4b11-b52b-5cacca3913d2",
   "metadata": {},
   "source": [
    "Parent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c2dee5-c36c-42d3-91ef-a1bfceed5eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from architectures.classifier import Classifier\n",
    "model = Classifier(backbone, embed_dim=256, mlp_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f5255-734a-403b-9b1e-23226a12a467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unfreezer] 4 layer group(s) queued for progressive unfreezing.\n"
     ]
    }
   ],
   "source": [
    "from trainer import fit\n",
    "\n",
    "# class imbalance: ~21935 neg / ~14873 pos ≈ 1.47\n",
    "model = fit(\n",
    "    model, train_loader, val_loader,\n",
    "    n_epochs=50,\n",
    "    lr=1e-4,\n",
    "    pos_weight=1.47,\n",
    "    unfreeze_groups=unfreeze_groups,\n",
    "    scheduler_patience=3,\n",
    "    unfreeze_patience=2,\n",
    "    unfreeze_lr_scale=0.5,\n",
    "    checkpoint_path=\"models/best_model_resnet152.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_fusion_env",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
